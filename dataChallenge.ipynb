{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44264ed1-44ab-4ab3-a408-af3c643fc908",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vischia/lisbon-ml-school/blob/master/dataChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc8d289-3918-452e-b6e1-28ed97caa8e4",
   "metadata": {},
   "source": [
    "# Lisbon Machine Learning School\n",
    "## Data Challenge!!! Multitarget regression\n",
    "\n",
    "(C) Pietro Vischia (Universidad de Oviedo and ICTEA), pietro.vischia@cern.ch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d49ac6-f688-46d1-82bb-c93ef1eb2cb6",
   "metadata": {},
   "source": [
    "## Setup the environment\n",
    "\n",
    "- If you are running locally, you don't need to run anything\n",
    "\n",
    "- If you are running on Google Colab, uncomment and run the next cell (remove only the \"#\", keep the \"!\"). You can also run it from a local installation, but it will do nothing if you have already installed all dependencies (and it will take some time to tell you it is not gonna do anything)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b9fe7a-5c11-4ce6-be45-402a10f052a0",
   "metadata": {},
   "source": [
    "## Load the needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c894a5d9-5ab1-442b-af11-13bcf3f79f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch version 2.6.0\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import torch.optim as optim \n",
    "from torch.utils.data import Dataset, DataLoader \n",
    "import torch.nn.functional as F \n",
    "import torchvision\n",
    "import torchinfo\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "from sklearn.metrics import roc_curve, auc, accuracy_score\n",
    "\n",
    "import uproot\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.rcParams['figure.figsize'] = (8, 6)\n",
    "matplotlib.rcParams['axes.labelsize'] = 14\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "\n",
    "print('Using torch version', torch.__version__)\n",
    "print('Using device:', device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b52b21-e0e5-4c2a-9afb-e42afb99e42f",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6cf761-a0b2-496c-83bc-5344dca6902d",
   "metadata": {},
   "source": [
    "We will use the same data we used for exercise 2, that is simulated events corresponding to three physics processes.\n",
    "- ttH production\n",
    "- ttW production\n",
    "- Drell-Yan ($pp\\\\to Z/\\\\gamma^*$+jets) production\n",
    "\n",
    "We will select the multilepton final state, which is a challenging final state with a rich structure and nontrivial background separation.\n",
    "\n",
    "<img src=\"figs/2lss.png\" alt=\"ttH multilepton 2lss\" style=\"width:40%\"/>\n",
    "\n",
    "We use the [uproot](https://uproot.readthedocs.io/en/latest/basic.html) library to conveniently read in a [ROOT TNuple](https://root.cern.ch/doc/master/classTNtuple.html) which can automatically convert it to a [pandas dataframe](https://pandas.pydata.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "74a60566-fd81-4c4b-b80c-1dafc8dc60c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the data only if you haven't done so yet\n",
    "\n",
    "# if not os.path.isfile(\"data/signal_blind20.root\"): \n",
    "#     !mkdir data; cd data/; wget https://www.hep.uniovi.es/vischia/lisbon_ml_school/lisbon_ml_school_tth.tar.gz; tar xzvf lisbon_ml_school_tth.tar.gz; rm lisbon_ml_school_tth.tar.gz; cd -;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c829b510-e956-477d-a841-81b0a369ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = uproot.open('data/signal_blind20.root')['Friends'].arrays(library=\"pd\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc7ae9-0d8e-4dac-b3bc-b8e986eb1654",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "Select the features you want to use for this exercise, don't forget to remove unnecessary features.\n",
    "\n",
    "Most of the variables are input features, corresponding to detector measurements of the properties of the reconstructed decay products.\n",
    "\n",
    "There are three special variables, though:\n",
    "\n",
    "- `Hreco_evt_tag`: this feature has values in ${0,1}$, where $1$ flags the event as signal event, and $0$ flags the event as background event;\n",
    "- `Hreco_HTXS_Higgs_pt`: this feature contains the true generate Higgs boson transverse momentum at generator level (used for regression);\n",
    "- `Hreco_HTXS_Higgs_y`: this feature contains the true generated Higgs boson rapidity (not pseudorapidity) at generator level (used for regression).\n",
    "\n",
    "\n",
    "### Important\n",
    "\n",
    "Twenty percent of the events have `-99` in the `Hreco_HTXS_Higgs_pt` and `Hreco_HTXS_Higgs_y` values. These are the \"unlabelled\" events that you will have to send predictions for. You should filter them out for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dec0a6-c8da-4201-b7a9-675be0612864",
   "metadata": {},
   "source": [
    "## The assignment\n",
    "\n",
    "- For this data challenge, your target is to simultaneously regress the Higgs transverse momentum `Hreco_HTXS_Higgs_pt` and the rapidity `Hreco_HTXS_Higgs_y`\n",
    "\n",
    "- You will need to split your dataset into two parts: one is where you have access to the pT and y labels (80% of the dataset): you will build your training and test sets from this. The other is where the pT and y has been set to -99: this is the portion of data that is kept blind. You will have to use 80% of the data to train a regressor, then evaluate the output of your regressor on the blind 20% of the data, and send us the results. We will compare the result with the true value we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "406b6ce3-0f17-4d42-a581-1ad77a019490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19994854437379506\n",
      "0.31147360226137455\n",
      "Index(['Hreco_Lep0_pt', 'Hreco_Lep1_pt', 'Hreco_HadTop_pt',\n",
      "       'Hreco_All5_Jets_pt', 'Hreco_More5_Jets_pt', 'Hreco_Jets_plus_Lep_pt',\n",
      "       'Hreco_Lep0_eta', 'Hreco_Lep1_eta', 'Hreco_HadTop_eta',\n",
      "       'Hreco_All5_Jets_eta', 'Hreco_More5_Jets_eta',\n",
      "       'Hreco_Jets_plus_Lep_eta', 'Hreco_Lep0_phi', 'Hreco_Lep1_phi',\n",
      "       'Hreco_HadTop_phi', 'Hreco_All5_Jets_phi', 'Hreco_More5_Jets_phi',\n",
      "       'Hreco_Jets_plus_Lep_phi', 'Hreco_Lep0_mass', 'Hreco_Lep1_mass',\n",
      "       'Hreco_HadTop_mass', 'Hreco_All5_Jets_mass', 'Hreco_More5_Jets_mass',\n",
      "       'Hreco_Jets_plus_Lep_mass', 'Hreco_TopScore', 'Hreco_met',\n",
      "       'Hreco_met_phi'],\n",
      "      dtype='object')\n",
      "Index(['Hreco_HTXS_Higgs_pt', 'Hreco_HTXS_Higgs_y'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop unneeded features\n",
    "\n",
    "blind_data = data[data[\"Hreco_HTXS_Higgs_pt\"]==-99]\n",
    "\n",
    "train_test_data = data[data[\"Hreco_HTXS_Higgs_pt\"]!=-99]\n",
    "train_test_data = data[data[\"Hreco_Lep2_pt\"]!=-99]\n",
    "\n",
    "print(blind_data.shape[0] / data.shape[0])\n",
    "print(train_test_data.shape[0] / data.shape[0])\n",
    "\n",
    "\n",
    "\n",
    "train_test_labels = train_test_data[[\"Hreco_HTXS_Higgs_pt\", \"Hreco_HTXS_Higgs_y\"]].copy()\n",
    "\n",
    "train_test_data = train_test_data.drop([\"index\", \"Hreco_HTXS_Higgs_pt\", \"Hreco_Lep2_phi\", \"Hreco_Lep2_mass\", \"Hreco_Lep2_eta\", \"Hreco_Lep2_pt\" , \"Hreco_HTXS_Higgs_y\", \"Hreco_evt_tag\"], axis=1 )\n",
    "\n",
    "# Filter data\n",
    "print(train_test_data.columns)\n",
    "print(train_test_labels.columns)\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler# standard scaling\n",
    ")\n",
    "\n",
    "# Scale the input features\n",
    "for column in train_test_data.columns:\n",
    "    scaler = StandardScaler().fit(train_test_data.filter([column], axis=1))\n",
    "    train_test_data[column] = scaler.transform(train_test_data.filter([column], axis=1))\n",
    "\n",
    "# Scale the labels\n",
    "for column in train_test_labels.columns:\n",
    "    scaler = StandardScaler().fit(train_test_labels.filter([column], axis=1))\n",
    "    train_test_labels[column] = scaler.transform(train_test_labels.filter([column], axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a72e29-98fb-41ac-b178-30402b03968a",
   "metadata": {},
   "source": [
    "- The loss function typically used for regression problems is the mean square error: in this case you will have to figure out how to deal with the fact that the output vector has dimension two (transverse momentum, and rapidity).\n",
    "- A tricky challenge is to deal with output features that have different scales: the rapidity is of $\\mathcal{O}(1)$, the transverse momentum is of $\\\\mathcal{O}(100-1000}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f772d79b-6dcd-4a2b-b9c1-5fb5b204771c",
   "metadata": {},
   "source": [
    "## Regression problems\n",
    "\n",
    "Regression problems require the prediction to be free of adopting the same range as the target variable(s) that need to be regressed.\n",
    "\n",
    "This is why the sigmoid activation function is not a good choice. The typical form of output layers of a regression problem is, if `n_outputs` is the dimension of the output vector:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1376288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([1024, 27])\n",
      "Labels batch shape: torch.Size([1024, 2])\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y, device=torch.device(\"cpu\")):\n",
    "        self.X = torch.Tensor(X.values).to(device)\n",
    "        self.y = torch.Tensor(y.values).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.y[idx]\n",
    "        datum = self.X[idx]\n",
    "        \n",
    "        return datum, label\n",
    "\n",
    "batch_size=1024 # Minibatch learning\n",
    "\n",
    "\n",
    "train_dataset = MyDataset(train_test_data, train_test_labels)\n",
    "# test_dataset = MyDataset(blind_data, y_test)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "78cbe240-aa59-40ce-9878-0877f6366a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=27, out_features=512, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Linear(in_features=64, out_features=8, bias=True)\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Linear(in_features=8, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Now let's see some more detailed info by using the torchinfo package\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NeuralNetwork                            [1024, 2]                 --\n",
       "├─Sequential: 1-1                        [1024, 2]                 --\n",
       "│    └─Linear: 2-1                       [1024, 512]               14,336\n",
       "│    └─LeakyReLU: 2-2                    [1024, 512]               --\n",
       "│    └─Linear: 2-3                       [1024, 128]               65,664\n",
       "│    └─LeakyReLU: 2-4                    [1024, 128]               --\n",
       "│    └─Linear: 2-5                       [1024, 64]                8,256\n",
       "│    └─LeakyReLU: 2-6                    [1024, 64]                --\n",
       "│    └─Linear: 2-7                       [1024, 8]                 520\n",
       "│    └─LeakyReLU: 2-8                    [1024, 8]                 --\n",
       "│    └─Linear: 2-9                       [1024, 2]                 18\n",
       "==========================================================================================\n",
       "Total params: 88,794\n",
       "Trainable params: 88,794\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 90.93\n",
       "==========================================================================================\n",
       "Input size (MB): 0.11\n",
       "Forward/backward pass size (MB): 5.85\n",
       "Params size (MB): 0.36\n",
       "Estimated Total Size (MB): 6.31\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, ninputs, device=torch.device(\"cpu\")):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(ninputs, 512),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(64,8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(8, 2),\n",
    "        )\n",
    "        self.linear_relu_stack.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass data through conv1\n",
    "        x = self.linear_relu_stack(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork(train_test_data.shape[1])\n",
    "\n",
    "print(model) # some basic info\n",
    "\n",
    "print(\"Now let's see some more detailed info by using the torchinfo package\")\n",
    "torchinfo.summary(model, input_size=(batch_size, train_test_data.shape[1])) # the input size is (batch size, number of features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5314e71e-c8f2-41ce-8107-370ed15f64fd",
   "metadata": {},
   "source": [
    "The other big change with respect to classification models is that the cross-entropy is not the proper loss function anymore.\n",
    "\n",
    "The regression problem is essentially a generalization of a linear regression problem, and the typical error estimates from classical statistics apply, each with its pros and cons.\n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "$MAE(\\hat{y}, y^{*}) = \\frac{1}{N} \\sum |\\hat{y} - y^{*}|$\n",
    "\n",
    "- Lower values are better.\n",
    "- It estimates the average error, thus cannot distinguish between one large error and many small errors.\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$RMSE(\\hat{y}, y^{*}) = \\sqrt{\\sum \\frac{(\\hat{y} - y^{*})^2}{N}}$\n",
    "\n",
    "- Lower values are better.\n",
    "- It estimates the spread of the residuals (standard deviation of the unexplained variance)\n",
    "- It gives large weight to large errors (if you use it as loss function, it will prioritize the reduction of large errors)\n",
    "\n",
    "#### Mean Absolute Percentage Error (MAPE)\n",
    "\n",
    "$MAPE(\\hat{y}, y^{*}) = \\frac{100\\%}{N} \\sum \\Big|\\frac{\\hat{y} - y^{*}}{y^{*}}\\Big|$\n",
    "\n",
    "#### R-Squared Score\n",
    "\n",
    "$R^2(\\hat{y}, y^{*}) = 1-\\frac{ \\sum (\\hat{y} - y^{*})^2}{  \\sum(\\bar{y} - y^{*})^2  }$, \n",
    "\n",
    "where $\\bar{y}$ is the arithmetic mean of the true values, $\\bar{y} = \\frac{1}{N}\\sum_{i=0}^{N-1} y^{*}$\n",
    "\n",
    "- It estimates how well the model explains the variance of the data\n",
    "- It can be negative (and that means that the model fits badly the data)\n",
    "\n",
    "\n",
    "You can consult online [an overview of the available loss functions in `pytorch`](https://pytorch.org/docs/stable/nn.html#loss-functions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd38be-7db1-4950-a55e-f1c17a0da3fd",
   "metadata": {},
   "source": [
    "## A few hints\n",
    "\n",
    "- Remove useless features\n",
    "- Consider the possibility of applying preprocessing to the input features, to the target features, or to both\n",
    "- Choose the appropriate metric to track\n",
    "- You can recycle the code for DataSet, DataLoader, Neural Network model, and train/test loops from exercise_1 essentially verbatim. Just make sure you change the loss function, and you change the output activation function to `nnReLU()`\n",
    "- Loss functions can be made as complicated as you want by defining your own loss function, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e77bdd1e-12d3-497f-bb5a-89402cd869c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef484b5-fd1d-41e7-b060-1ff4f2c35f15",
   "metadata": {},
   "source": [
    "## The scoring system\n",
    "\n",
    "- You will have to define a model with two output nodes: the first one must regress the Higgs boson transverse momentum, the second one must regress the Higgs boson rapidity.\n",
    "- You can also use any flavour of boosted decision trees you may see fit, but implemented in `torch`.\n",
    "- You will have to evaluate your model on the unlabelled data, save the predictions to a csv file with commas as separators (format: pt, y), and send us the csv file (see below). \n",
    "- If you have filtered the features further, please include in the email the code that creates the `data` dataframe.\n",
    "\n",
    "We will evaluate the results of the challenge on the unlabelled events, using as performance metric the RMSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "41f61a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, best_model_path, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    losses=[] # Track the loss function\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    #for batch, (X, y) in enumerate(dataloader):\n",
    "    best_loss = np.inf\n",
    "    for (X,y) in tqdm(dataloader):\n",
    "        # Reset gradients (to avoid their accumulation)\n",
    "        optimizer.zero_grad()\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        #if (all_equal3(pred.detach().numpy())):\n",
    "        #    print(\"All equal!\")\n",
    "        loss = loss_fn(pred.squeeze(dim=1), y)\n",
    "        losses.append(loss.detach().cpu())\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss.detach().cpu()\n",
    "            torch.save(model.state_dict(), best_model_path) # Save the full state of the model, to have access to the training history\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    losses=[] # Track the loss function\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        #for X, y in dataloader:\n",
    "        for (X,y) in tqdm(dataloader):\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred.squeeze(dim=1), y).item()\n",
    "            losses.append(loss)\n",
    "            test_loss += loss\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ea4ff88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 128.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9276975 , Current learning rate [0.01]\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 109.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9267818 , Current learning rate [0.01]\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 148.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.92432743 , Current learning rate [0.01]\n",
      "Epoch 4\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 117.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9295093 , Current learning rate [0.01]\n",
      "Epoch 5\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 145.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.93263227 , Current learning rate [0.01]\n",
      "Epoch 6\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 128.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9215499 , Current learning rate [0.01]\n",
      "Epoch 7\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 130.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.92408496 , Current learning rate [0.01]\n",
      "Epoch 8\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 156.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.91899955 , Current learning rate [0.01]\n",
      "Epoch 9\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 137.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9177199 , Current learning rate [0.01]\n",
      "Epoch 10\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 125.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.91802293 , Current learning rate [0.009000000000000001]\n",
      "Epoch 11\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 161.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.91441894 , Current learning rate [0.009000000000000001]\n",
      "Epoch 12\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 128.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.91637534 , Current learning rate [0.009000000000000001]\n",
      "Epoch 13\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 131.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9148917 , Current learning rate [0.009000000000000001]\n",
      "Epoch 14\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 162.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.90902513 , Current learning rate [0.009000000000000001]\n",
      "Epoch 15\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.90640914 , Current learning rate [0.009000000000000001]\n",
      "Epoch 16\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 150.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.90678346 , Current learning rate [0.009000000000000001]\n",
      "Epoch 17\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9127884 , Current learning rate [0.009000000000000001]\n",
      "Epoch 18\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.91029507 , Current learning rate [0.009000000000000001]\n",
      "Epoch 19\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 160.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9085136 , Current learning rate [0.009000000000000001]\n",
      "Epoch 20\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 137.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.90681756 , Current learning rate [0.008100000000000001]\n",
      "Epoch 21\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 140.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.9001351 , Current learning rate [0.008100000000000001]\n",
      "Epoch 22\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 151.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.896889 , Current learning rate [0.008100000000000001]\n",
      "Epoch 23\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.89038026 , Current learning rate [0.008100000000000001]\n",
      "Epoch 24\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 162.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.88672686 , Current learning rate [0.008100000000000001]\n",
      "Epoch 25\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 139.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.88854957 , Current learning rate [0.008100000000000001]\n",
      "Epoch 26\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 137.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.8846306 , Current learning rate [0.008100000000000001]\n",
      "Epoch 27\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 163.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.8746418 , Current learning rate [0.008100000000000001]\n",
      "Epoch 28\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 131.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.87674034 , Current learning rate [0.008100000000000001]\n",
      "Epoch 29\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.87157786 , Current learning rate [0.008100000000000001]\n",
      "Epoch 30\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 164.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.8636153 , Current learning rate [0.007290000000000001]\n",
      "Epoch 31\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 139.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.8526183 , Current learning rate [0.007290000000000001]\n",
      "Epoch 32\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 139.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.85266775 , Current learning rate [0.007290000000000001]\n",
      "Epoch 33\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 163.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.84760725 , Current learning rate [0.007290000000000001]\n",
      "Epoch 34\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 129.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.84613675 , Current learning rate [0.007290000000000001]\n",
      "Epoch 35\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 163.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.84729975 , Current learning rate [0.007290000000000001]\n",
      "Epoch 36\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 139.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.83724207 , Current learning rate [0.007290000000000001]\n",
      "Epoch 37\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 139.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.840595 , Current learning rate [0.007290000000000001]\n",
      "Epoch 38\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 163.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.8267638 , Current learning rate [0.007290000000000001]\n",
      "Epoch 39\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.8273262 , Current learning rate [0.007290000000000001]\n",
      "Epoch 40\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 123.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.8168149 , Current learning rate [0.006561000000000002]\n",
      "Epoch 41\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 163.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.8181231 , Current learning rate [0.006561000000000002]\n",
      "Epoch 42\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 138.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.7938008 , Current learning rate [0.006561000000000002]\n",
      "Epoch 43\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 164.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.78928393 , Current learning rate [0.006561000000000002]\n",
      "Epoch 44\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 135.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.79591405 , Current learning rate [0.006561000000000002]\n",
      "Epoch 45\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 139.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.78783673 , Current learning rate [0.006561000000000002]\n",
      "Epoch 46\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 150.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.7826248 , Current learning rate [0.006561000000000002]\n",
      "Epoch 47\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 139.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.7667225 , Current learning rate [0.006561000000000002]\n",
      "Epoch 48\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 138.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.76436347 , Current learning rate [0.006561000000000002]\n",
      "Epoch 49\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 163.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.7620945 , Current learning rate [0.006561000000000002]\n",
      "Epoch 50\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 137.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.75079936 , Current learning rate [0.005904900000000002]\n",
      "Epoch 51\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.7367914 , Current learning rate [0.005904900000000002]\n",
      "Epoch 52\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 152.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.7301748 , Current learning rate [0.005904900000000002]\n",
      "Epoch 53\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 135.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.71577096 , Current learning rate [0.005904900000000002]\n",
      "Epoch 54\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 160.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.73011905 , Current learning rate [0.005904900000000002]\n",
      "Epoch 55\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 138.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.7039242 , Current learning rate [0.005904900000000002]\n",
      "Epoch 56\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.69815737 , Current learning rate [0.005904900000000002]\n",
      "Epoch 57\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 160.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.68922746 , Current learning rate [0.005904900000000002]\n",
      "Epoch 58\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.677757 , Current learning rate [0.005904900000000002]\n",
      "Epoch 59\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 128.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.6795253 , Current learning rate [0.005904900000000002]\n",
      "Epoch 60\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 162.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.6752964 , Current learning rate [0.005314410000000002]\n",
      "Epoch 61\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 137.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.6601295 , Current learning rate [0.005314410000000002]\n",
      "Epoch 62\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 164.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.64632493 , Current learning rate [0.005314410000000002]\n",
      "Epoch 63\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 135.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.6379517 , Current learning rate [0.005314410000000002]\n",
      "Epoch 64\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.635872 , Current learning rate [0.005314410000000002]\n",
      "Epoch 65\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 151.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.63751364 , Current learning rate [0.005314410000000002]\n",
      "Epoch 66\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 130.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.6286251 , Current learning rate [0.005314410000000002]\n",
      "Epoch 67\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 138.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.61007506 , Current learning rate [0.005314410000000002]\n",
      "Epoch 68\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 164.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.6269763 , Current learning rate [0.005314410000000002]\n",
      "Epoch 69\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 138.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.6204633 , Current learning rate [0.005314410000000002]\n",
      "Epoch 70\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 138.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.60685515 , Current learning rate [0.004782969000000002]\n",
      "Epoch 71\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 151.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5958604 , Current learning rate [0.004782969000000002]\n",
      "Epoch 72\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 138.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5746805 , Current learning rate [0.004782969000000002]\n",
      "Epoch 73\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 158.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5717991 , Current learning rate [0.004782969000000002]\n",
      "Epoch 74\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 137.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.56569284 , Current learning rate [0.004782969000000002]\n",
      "Epoch 75\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 137.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5707613 , Current learning rate [0.004782969000000002]\n",
      "Epoch 76\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 160.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.56699896 , Current learning rate [0.004782969000000002]\n",
      "Epoch 77\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 130.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.58264124 , Current learning rate [0.004782969000000002]\n",
      "Epoch 78\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 123.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5757586 , Current learning rate [0.004782969000000002]\n",
      "Epoch 79\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 145.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.56187373 , Current learning rate [0.004782969000000002]\n",
      "Epoch 80\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 122.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5403446 , Current learning rate [0.004304672100000002]\n",
      "Epoch 81\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 144.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.54088235 , Current learning rate [0.004304672100000002]\n",
      "Epoch 82\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 126.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5357312 , Current learning rate [0.004304672100000002]\n",
      "Epoch 83\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 115.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5475222 , Current learning rate [0.004304672100000002]\n",
      "Epoch 84\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 148.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.52812344 , Current learning rate [0.004304672100000002]\n",
      "Epoch 85\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 128.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.52857286 , Current learning rate [0.004304672100000002]\n",
      "Epoch 86\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 125.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.5206157 , Current learning rate [0.004304672100000002]\n",
      "Epoch 87\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 118.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.51248664 , Current learning rate [0.004304672100000002]\n",
      "Epoch 88\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 121.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.50138605 , Current learning rate [0.004304672100000002]\n",
      "Epoch 89\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 136.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.4965754 , Current learning rate [0.004304672100000002]\n",
      "Epoch 90\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 160.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.50165635 , Current learning rate [0.003874204890000002]\n",
      "Epoch 91\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 135.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.49525696 , Current learning rate [0.003874204890000002]\n",
      "Epoch 92\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 159.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.4828223 , Current learning rate [0.003874204890000002]\n",
      "Epoch 93\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 126.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.48937172 , Current learning rate [0.003874204890000002]\n",
      "Epoch 94\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 129.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.4811533 , Current learning rate [0.003874204890000002]\n",
      "Epoch 95\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 159.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.48917344 , Current learning rate [0.003874204890000002]\n",
      "Epoch 96\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 133.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.47606555 , Current learning rate [0.003874204890000002]\n",
      "Epoch 97\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 126.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.48867399 , Current learning rate [0.003874204890000002]\n",
      "Epoch 98\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 146.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.4760581 , Current learning rate [0.003874204890000002]\n",
      "Epoch 99\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 119.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.47295973 , Current learning rate [0.003874204890000002]\n",
      "Epoch 100\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 92/92 [00:00<00:00, 140.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg train loss 0.46975583 , Current learning rate [0.003486784401000002]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "learningRate = 0.01\n",
    "\n",
    "# The optimizer decides which path to follow through the gradient of the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "# The scheduler reduces the learning rate for the optimizer in order to for the optimizer to be able to \"enter\" narrow minima\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 10, gamma=0.9)\n",
    "\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "best_model_path = \"best_dnn_model.h5\"\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss=train_loop(train_dataloader, model, loss_fn, optimizer, scheduler, best_model_path, device)\n",
    "    # test_loss=test_loop(test_dataloader, model, loss_fn, device)\n",
    "    train_losses.append(train_loss)\n",
    "    # test_losses.append(test_loss)\n",
    "    print(\"Avg train loss\", train_loss, \", Current learning rate\", scheduler.get_last_lr())\n",
    "    # print(\"Avg train loss\", train_loss, \", Avg test loss\", test_loss, \"Current learning rate\", scheduler.get_last_lr())\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bcbc4170-4898-477f-acab-125aecbb9da3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'blind_data_X_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[137], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m to_send \u001b[38;5;241m=\u001b[39m model(\u001b[43mblind_data_X_features\u001b[49m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m df_to_send \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(to_send)\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_file_group_N\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m# Replace \"N\" with the group number we assigned you\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'blind_data_X_features' is not defined"
     ]
    }
   ],
   "source": [
    "to_send = model(blind_data_X_features).cpu().numpy()\n",
    "\n",
    "df_to_send = pd.DataFrame(to_send)\n",
    "df.to_csv(\"my_file_group_N\",index=False) # Replace \"N\" with the group number we assigned you\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a22ec80-956b-45ab-9e0d-58325019a516",
   "metadata": {},
   "source": [
    "# Practical instructions\n",
    "\n",
    "- Divide yourselves in groups of three. Once the groups are defined, we will assign you a numerical label.\n",
    "- Each group will have to send the csv file mentioned above as an attached `.csv`file to [lisbon-ml-workshop@cern.ch](mailto:lisbon-ml-workshop@cern.ch). The email (ONE EMAIL PER GROUP) must have:\n",
    "  - As an object: \"LIP ML Workshop Data Challenge: Group N\", where \"N\" is the number we assigned you above\n",
    "  - All the three members of the group must be in carbon copy to the email\n",
    "  - In the email text there must be a list of the full names of the three members of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e742f57-0770-4043-b0e0-b50441146557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lisbon_ml_school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
